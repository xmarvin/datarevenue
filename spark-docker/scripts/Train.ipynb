{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import pyspark as spark\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark import SQLContext\n",
    "import os\n",
    "import csv\n",
    "from io import StringIO\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from nltk.stem.snowball import SnowballStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName('app').setMaster('local')\n",
    "sc = SparkContext(conf=conf)\n",
    "hc = SparkSession(sc)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = sqlContext.read.csv(\"small/train.csv\", header=True)\n",
    "train = train.withColumn(\"group\", train['group'].cast('int'))\n",
    "train = train.withColumn(\"price\", train['price'].cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"description\", outputCol=\"words\")\n",
    "#wordsData = tokenizer.transform(train)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=2500)\n",
    "#tf = hashingTF.transform(wordsData)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "#idfModel = idf.fit(tf)\n",
    "#tfidf = idfModel.transform(tf)\n",
    "assembler = VectorAssembler(inputCols=[\"price\",\"features\"],handleInvalid = \"skip\",outputCol=\"all_features\")\n",
    "#tfidf = assembler.transform(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol=\"all_features\", labelCol='group', regParam=0.1)\n",
    "#lrModel = lr.fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, assembler, lr])\n",
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_train = model.transform(train)\n",
    "#res_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"lr.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#res_train.select(['group','rawPrediction','probability','prediction']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def row2csv(row):\n",
    "#     buffer = StringIO()\n",
    "#     writer = csv.writer(buffer)\n",
    "#     writer.writerow([str(s).encode(\"utf-8\") for s in row])\n",
    "#     buffer.seek(0)\n",
    "#     return buffer.read().strip()\n",
    "\n",
    "# res_train.select(['group','rawPrediction','probability','prediction']).rdd.map(row2csv).coalesce(1).saveAsTextFile(\"pred_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8723837999456374\n"
     ]
    }
   ],
   "source": [
    "def evaluate_results(res):    \n",
    "    res = res.withColumn(\"group\", res['group'].cast('float'))\n",
    "    metrics = MulticlassMetrics(res.select(['group','prediction']).rdd)\n",
    "    print(\"Accuracy = %s\" % metrics.accuracy)\n",
    "evaluate_results(res_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = sqlContext.read.csv(\"small/test.csv\", header=True)\n",
    "test = test.withColumn(\"group\", test['group'].cast('int'))\n",
    "test = test.withColumn(\"price\", test['price'].cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "model2 = PipelineModel.load('lr.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.6801258521237545\n"
     ]
    }
   ],
   "source": [
    "res_test = model2.transform(test)\n",
    "evaluate_results(res_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
